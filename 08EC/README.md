# Edge Computing

## Software

### AutoML

#### AutoML

**Amc: Automl for model compression and acceleration on mobile devices.**<br>
*Y He, J Lin, Z Liu, H Wang, LJ Li.*<br>
CVPR, 2018.

**Auto-keras: An efficient neural architecture search system.**<br>
*H Jin, Q Song, X Hu.*<br>
KDD, 2019.

**Auto-sklearn 2.0: The next generation.**<br>
*JM Feurer, K Eggensperger, S Falkner, et al.*<br>
ArXiv, 2020.

**Vega: towards an end-to-end configurable automl pipeline.**<br>
*JB Wang, H Xu, J Zhang, C Chen, X Fang, Y Xu, et al.*<br>
ArXiv, 2020.

**AutoML A survey of the state-of-the-art.**<br>
*X He, K Zhao, X Chu.*<br>
Knowledge-Based Systems, 2021.

**Neural Network Intelligence.**<br>
[[Github](https://github.com/microsoft/nni)]

#### Neural Architecture Search (NAS)

**Regularized evolution for image classifier architecture search.**<br>
*E Real, A Aggarwal, Y Huang, QV Le.*<br>
AAAI, 2019.

**Renas: Reinforced evolutionary neural architecture search.**<br>
*Y Chen, G Meng, Q Zhang, S Xiang.*<br>
CVPR, 2019.

**Darts: Differentiable architecture search.**<br>
*H Liu, K Simonyan, Y Yang.*<br>
ICLR, 2019.

**Neural architecture search A survey.**<br>
*T Elsken, JH Metzen, F Hutter.*<br>
JLMR, 2019.

**A survey on evolutionary neural architecture search.**<br>
*Y Liu, Y Sun, B Xue, M Zhang, GG Yen, et al.*<br>
TNNLS, 2020.

**Apq: Joint search for network architecture, pruning and quantization policy.**<br>
*T Wang, K Wang, H Cai, J Lin, Z Liu.*<br>
CVPR, 2020.

**A comprehensive survey of neural architecture search: Challenges and solutions.**<br>
*P Ren, Y Xiao, X Chang, PY Huang, Z Li, et al.*<br>
CUSR, 2021.

**A survey on evolutionary neural architecture search.**<br>
*Y Liu, Y Sun, B Xue, M Zhang, GG Yen, et al.*<br>
IEEE Transactions on Neural Networks and Learning Systems, 2021.

#### MLOps

**Building continuous integration services for machine learning.**<br>
*B Karlaš, M Interlandi, C Renggli, W Wu, et al.*<br>
KDD, 2020.

**A data quality-driven view of mlops.**<br>
*C Renggli, L Rimanic, NM Gürel, B Karlaš, et al.*<br>
ArXiv, 2021.

**kubeflow.**<br>
*Machine Learning Toolkit for Kubernetes.*<br>
[[Github](https://github.com/kubeflow/kubeflow)]

**mlflow.**<br>
*Open source platform for the machine learning lifecycle.*<br>
[[Github](https://github.com/mlflow/mlflow)]

### Light Models

#### Lightweigh Models

**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size.**<br>
*FN Iandola, S Han, MW Moskewicz, K Ashraf, et al.*<br>
ArXiv, 2016.

**Xnor-net: Imagenet classification using binary convolutional neural networks.**<br>
*M Rastegari, V Ordonez, J Redmon, et al.*<br>
ECCV, 2016.

**Mobilenets: Efficient convolutional neural networks for mobile vision applications.**<br>
*AG Howard, M Zhu, B Chen, D Kalenichenko, et al.*<br>
ArXiv, 2017.

**Mobilenetv2: Inverted residuals and linear bottlenecks.**<br>
*M Sandler, A Howard, M Zhu, et al.*<br>
CVPR, 2018.

**Shufflenet: An extremely efficient convolutional neural network for mobile devices.**<br>
*X Zhang, X Zhou, M Lin, J Sun.*<br>
CVPR, 2018.

**Searching for mobilenetv3.**<br>
*A Howard, M Sandler, G Chu, et al.*<br>
CVPR, 2019.

**SlimYOLOv3: Narrower, faster and better for real-time UAV applications.**<br>
*P Zhang, Y Zhong, X Li.*<br>
CVPR, 2019.

**Efficientnet: Rethinking model scaling for convolutional neural networks.**<br>
*M Tan, Q Le.*<br>
ICML, 2019.

**Yolov4: Optimal speed and accuracy of object detection.**<br>
*A Bochkovskiy, CY Wang, HYM Liao.*<br>
ArXiv, 2020.

**Efficientdet: Scalable and efficient object detection.**<br>
*M Tan, R Pang, QV Le, et al.*<br>
CVPR, 2020.

#### Model Compression

**Learning Efficient Convolutional Networks Through Network Slimming.**<br>
*Z Liu, J Li, Z Shen, G Huang, S Yan, et al.*<br>
ICCV, 2017.
[[Github](https://github.com/tanluren/yolov3-channel-and-layer-pruning)]

**Model Compression and Hardware Acceleration for Neural Networks A Comprehensive Survey.**<br>
*L Deng, G Li, S Han, L Shi, Y Xie.*<br> 
Proceedings of the IEEE, 2020.

**A comprehensive survey on model compression and acceleration.**<br>
*T Choudhary, V Mishra, A Goswami, et al.*<br>
Artificial Intelligence Review, 2020.

#### Model Deployment

**Efficient inference with tensorrt.**<br>
*H Vanholder.*<br>
GPU Technology Conference, 2016.

**Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference.**<br>
*Y Gorbachev, M Fedorov, I Slavutin, et al.*<br>
ICCV, 2019.

**mmdeploy.**<br>
*OpenMMLab Model Deployment Framework.*<br>
[[Github](https://github.com/open-mmlab/mmdeploy)]

**MNN.**<br>
*MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba.*<br>
[[Github](https://github.com/alibaba/MNN)]

**NCNN.**<br>
*ncnn is a high-performance neural network inference framework optimized for the mobile platform.*<br>
[[Github](https://github.com/Tencent/ncnn)]

### Training Accelerations

#### Few Shot Learning

**Few-shot Object Detection via Feature Reweighting.**<br>
*B Kang, Z Liu, X Wang, F Yu, et al.*<br>
ICCV, 2019.
[[Github](https://github.com/bingykang/Fewshot_Detection)]

**Generalizing from a few examples A survey on few-shot learning.**<br>
*Y Wang, Q Yao, JT Kwok, LM Ni.*<br>
CSUR, 2020.

**Meta-learning of neural architectures for few-shot learning.**<br>
*T Elsken, B Staffler, JH Metzen, et al.*<br>
CVPR, 2020.

**Meta-Learning-Papers**<br> 
[[Github](https://github.com/floodsung/Meta-Learning-Papers)]

#### Data Annotation/Labeling

**Semi-Automated Data Labeling.**<br>
*M Desmond, E Duesterwald，et al.*<br>
NeurIPS, 2020.

**Semi-automatic data annotation guided by feature space projection.**<br>
*BC Benato, JF Gomes, AC Telea, AX Falcão.*<br>
Pattern Recognition, 2021.

#### Efficient Training

**TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning.**<br>
*H Cai, C Gan, L Zhu, S Han.*<br>
NeurIPS, 2020.

**A comprehensive survey on transfer learning.**<br>
*F Zhuang, Z Qi, K Duan, D Xi, Y Zhu, et al.*<br>
Proceedings of the IEEE, 2020.

## Hardware-aware

**Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.**<br>
*B Wu, X Dai, P Zhang, Y Wang, F Sun, et al.*<br>
CVPR, 2019.

**Haq: Hardware-aware automated quantization with mixed precision.**<br>
*K Wang, Z Liu, Y Lin, J Lin, et al.*<br>
CVPR, 2019.

**Hat: Hardware-aware transformers for efficient natural language processing.**<br>
*H Wang, Z Wu, Z Liu, H Cai, L Zhu, C Gan, et al.*<br>
ACL, 2020.

**Once-for-all: Train one network and specialize it for efficient deployment.**<br>
*H Cai, C Gan, T Wang, Z Zhang, S Han.*<br>
ICLR, 2020.

**HW-NAS-Bench: Hardware-aware neural architecture search benchmark.**<br>
*C Li, Z Yu, Y Fu, Y Zhang, Y Zhao, H You, Q Yu, et al.*<br>
ICLR, 2021.

**Hardware-Aware Neural Architecture Search: Survey and Taxonomy.**<br>
*H Benmeziane, K El Maghraoui, et al.*<br>
IJCAI, 2021.

