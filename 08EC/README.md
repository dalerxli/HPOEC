# Edge Computing

## Efficient Machine Learning Algorithms (EMLA)

### MLFlow

#### AutoML

##### AML

**Amc: Automl for model compression and acceleration on mobile devices.**<br>
*Y He, J Lin, Z Liu, H Wang, LJ Li.*<br>
CVPR, 2018.

**Auto-keras: An efficient neural architecture search system.**<br>
*H Jin, Q Song, X Hu.*<br>
KDD, 2019.[[Github](https://github.com/keras-team/autokeras)]

**Autogluon-tabular: Robust and accurate automl for structured data.**<br>
*JN Erickson, J Mueller, A Shirkov, H Zhang, et al.*<br>
ArXiv, 2020.
[[Github](https://github.com/awslabs/autogluon)]

**Auto-sklearn 2.0: The next generation.**<br>
*JM Feurer, K Eggensperger, S Falkner, et al.*<br>
ArXiv, 2020.

**Vega: towards an end-to-end configurable automl pipeline.**<br>
*JB Wang, H Xu, J Zhang, C Chen, X Fang, Y Xu, et al.*<br>
ArXiv, 2020.
[[Github](https://github.com/vega/vega)]

**AutoML A survey of the state-of-the-art.**<br>
*X He, K Zhao, X Chu.*<br>
Knowledge-Based Systems, 2021.

**AutoSNN Towards Energy-Efficient Spiking Neural Networks.**<br>
*B Na, J Mok, S Park, D Lee, H Choe, S Yoon.*<br>
ArXiv, 2022.

**windmaple/awesome-AutoML.**<br>
*Curating a list of AutoML-related research, tools, projects and other resources.*<br>
[[Github](https://github.com/hibayesian/awesome-automl-papers)]

**hibayesian/awesome-automl-papers.**<br>
*A curated list of automated machine learning papers, articles, tutorials, slides and projects.*<br>
[[Github](https://github.com/microsoft/FLAML)]

**microsoft/FLAML.**<br>
*A fast library for AutoML and tuning.*<br>
[[Github](https://github.com/microsoft/FLAML)]

**microsoft/nni.**<br>
*An open source AutoML toolkit for automate machine learning lifecycle, including feature engineering, neural architecture search, model compression and hyper-parameter tuning.*<br>
[[Github](https://github.com/microsoft/nni)]

##### NAS

**Neural architecture search with reinforcement learning.**<br>
*B Zoph, QV Le.*<br>
ArXiv, 2016.

**Progressive neural architecture search.**<br>
*C Liu, B Zoph, M Neumann, J Shlens, at al.*<br>
ICML, 2018.

**Efficient neural architecture search via parameters sharing.**<br>
*H Pham, M Guan, B Zoph, Q Le, et al.*<br>
ECCV, 2018.

**Regularized evolution for image classifier architecture search.**<br>
*E Real, A Aggarwal, Y Huang, QV Le.*<br>
AAAI, 2019.

**Renas: Reinforced evolutionary neural architecture search.**<br>
*Y Chen, G Meng, Q Zhang, S Xiang.*<br>
CVPR, 2019.

**Darts: Differentiable architecture search.**<br>
*H Liu, K Simonyan, Y Yang.*<br>
ICLR, 2019.

**Deepswarm: Optimising convolutional neural networks using swarm intelligence.**<br>
*E Byla, W Pang.*<br>
UK Workshop on Computational Intelligence, 2019.

**Neural architecture search A survey.**<br>
*T Elsken, JH Metzen, F Hutter.*<br>
JLMR, 2019.

**A survey on evolutionary neural architecture search.**<br>
*Y Liu, Y Sun, B Xue, M Zhang, GG Yen, et al.*<br>
TNNLS, 2020.

**Apq: Joint search for network architecture, pruning and quantization policy.**<br>
*T Wang, K Wang, H Cai, J Lin, Z Liu.*<br>
CVPR, 2020.

**A comprehensive survey of neural architecture search: Challenges and solutions.**<br>
*P Ren, Y Xiao, X Chang, PY Huang, Z Li, et al.*<br>
CUSR, 2021.

**A survey on evolutionary neural architecture search.**<br>
*Y Liu, Y Sun, B Xue, M Zhang, GG Yen, et al.*<br>
IEEE Transactions on Neural Networks and Learning Systems, 2021.

**joeddav/devol.**<br>
*Genetic neural architecture search with Keras.*<br>
[[Github](https://github.com/joeddav/devol)]

**deephyper/deephyper.**<br>
*DeepHyper: Scalable Asynchronous Neural Architecture and Hyperparameter Search for Deep Neural Networks.*<br>
[[Github](https://github.com/deephyper/deephyper)]

##### Hardware-aware Design

**Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.**<br>
*B Wu, X Dai, P Zhang, Y Wang, F Sun, et al.*<br>
CVPR, 2019.

**Haq: Hardware-aware automated quantization with mixed precision.**<br>
*K Wang, Z Liu, Y Lin, J Lin, et al.*<br>
CVPR, 2019.

**Proxylessnas: Direct neural architecture search on target task and hardware.**<br>
*H Cai, L Zhu, S Han.*<br>
ICLR, 2019.

**Hat: Hardware-aware transformers for efficient natural language processing.**<br>
*H Wang, Z Wu, Z Liu, H Cai, L Zhu, C Gan, et al.*<br>
ACL, 2020.

**Once-for-all: Train one network and specialize it for efficient deployment.**<br>
*H Cai, C Gan, T Wang, Z Zhang, S Han.*<br>
ICLR, 2020.

**HW-NAS-Bench: Hardware-aware neural architecture search benchmark.**<br>
*C Li, Z Yu, Y Fu, Y Zhang, Y Zhao, H You, Q Yu, et al.*<br>
ICLR, 2021.

**Hardware-Aware Neural Architecture Search: Survey and Taxonomy.**<br>
*H Benmeziane, K El Maghraoui, et al.*<br>
IJCAI, 2021.

#### MLOps

**Building continuous integration services for machine learning.**<br>
*B Karla≈°, M Interlandi, C Renggli, W Wu, et al.*<br>
KDD, 2020.

**A data quality-driven view of mlops.**<br>
*C Renggli, L Rimanic, NM G√ºrel, B Karla≈°, et al.*<br>
ArXiv, 2021.

**TinyMLOps: Operational Challenges for Widespread Edge AI Adoption.**<br>
*S Leroux, P Simoens, M Lootus, K Kathore, et al.*<br>
ArXiv, 2022.

**visenger/awesome-mlops.**<br>
*A curated list of references for MLOps.*<br>
[[Github](https://github.com/visenger/awesome-mlops)]

**kelvins/awesome-mlops.**<br>
*A curated list of awesome MLOps tools.*<br>
[[Github](https://github.com/kelvins/awesome-mlops)]

**zenml-io/zenml.**<br>
*ZenML üôè: MLOps framework to create reproducible pipelines.*<br>
[[Github](https://github.com/zenml-io/zenml)]

**kubeflow.**<br>
*Machine Learning Toolkit for Kubernetes.*<br>
[[Github](https://github.com/kubeflow/kubeflow)]

**mlflow.**<br>
*Open source platform for the machine learning lifecycle.*<br>
[[Github](https://github.com/mlflow/mlflow)]

#### Model Deployment

**Efficient inference with tensorrt.**<br>
*H Vanholder.*<br>
GPU Technology Conference, 2016.

**TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.**<br>
*T Chen, T Moreau, Z Jiang, et al.*<br>
OSDI, 2018.

**Openvino deep learning workbench: Comprehensive analysis and tuning of neural networks inference.**<br>
*Y Gorbachev, M Fedorov, I Slavutin, et al.*<br>
ICCV, 2019.

**Once-for-all: Train one network and specialize it for efficient deployment.**<br>
*H Cai, C Gan, T Wang, Z Zhang, S Han.*<br>
ICLR, 2020.

**An empirical study on deployment faults of deep learning based mobile applications.**<br>
*Z Chen, H Yao, Y Lou, et al.*<br>
ICSE, 2021.

**ahkarami/Deep-Learning-in-Production.**<br>
*In this repository, I will share some useful notes and references about deploying deep learning-based models in production.*<br>
[[Github](https://github.com/ahkarami/Deep-Learning-in-Production)]

**mmdeploy.**<br>
*OpenMMLab Model Deployment Framework.*<br>
[[Github](https://github.com/open-mmlab/mmdeploy)]

**MNN.**<br>
*MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba.*<br>
[[Github](https://github.com/alibaba/MNN)]

**NCNN.**<br>
*ncnn is a high-performance neural network inference framework optimized for the mobile platform.*<br>
[[Github](https://github.com/Tencent/ncnn)]

### Efficient Models

#### Lightweight Models

**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size.**<br>
*FN Iandola, S Han, MW Moskewicz, K Ashraf, et al.*<br>
ArXiv, 2016.

**Xnor-net: Imagenet classification using binary convolutional neural networks.**<br>
*M Rastegari, V Ordonez, J Redmon, et al.*<br>
ECCV, 2016.

**Mobilenets: Efficient convolutional neural networks for mobile vision applications.**<br>
*AG Howard, M Zhu, B Chen, D Kalenichenko, et al.*<br>
ArXiv, 2017.

**Mobilenetv2: Inverted residuals and linear bottlenecks.**<br>
*M Sandler, A Howard, M Zhu, et al.*<br>
CVPR, 2018.

**Shufflenet: An extremely efficient convolutional neural network for mobile devices.**<br>
*X Zhang, X Zhou, M Lin, J Sun.*<br>
CVPR, 2018.

**Searching for mobilenetv3.**<br>
*A Howard, M Sandler, G Chu, et al.*<br>
CVPR, 2019.

**SlimYOLOv3: Narrower, faster and better for real-time UAV applications.**<br>
*P Zhang, Y Zhong, X Li.*<br>
CVPR, 2019.

**Efficientnet: Rethinking model scaling for convolutional neural networks.**<br>
*M Tan, Q Le.*<br>
ICML, 2019.

**Yolov4: Optimal speed and accuracy of object detection.**<br>
*A Bochkovskiy, CY Wang, HYM Liao.*<br>
ArXiv, 2020.

**Efficientdet: Scalable and efficient object detection.**<br>
*M Tan, R Pang, QV Le, et al.*<br>
CVPR, 2020.

**guan-yuan/awesome-AutoML-and-Lightweight-Models.**<br>
*A list of high-quality (newest) AutoML works and lightweight models including 1.) Neural Architecture Search, 2.) Lightweight Structures, 3.) Model Compression, Quantization and Acceleration, 4.) Hyperparameter Optimization, 5.) Automated Feature Engineering.*<br>
[[Github](https://github.com/guan-yuan/awesome-AutoML-and-Lightweight-Models)]

**DefTruth/lite.ai.toolkit.**<br>
*üõ† A lite C++ toolkit of awesome AI models with ONNXRuntime, NCNN, MNN and TNN. YOLOX, YOLOP, MODNet, YOLOR, NanoDet, YOLOX, SCRFD, YOLOX . MNN, NCNN, TNN, ONNXRuntime, CPU/GPU.*<br>
[[Github](DefTruth/lite.ai.toolkit)]

#### Model Compression

**Learning Efficient Convolutional Networks Through Network Slimming.**<br>
*Z Liu, J Li, Z Shen, G Huang, S Yan, et al.*<br>
ICCV, 2017.
[[Github](https://github.com/tanluren/yolov3-channel-and-layer-pruning)]

**Quantization and training of neural networks for efficient integer-arithmetic-only inference.**<br>
*B Jacob, S Kligys, B Chen, M Zhu, et al.*<br>
CVPR, 2018.

**Similarity-preserving knowledge distillation.**<br>
*F Tung, G Mori.*<br> 
ICCV, 2019.

**A comprehensive survey on model compression and acceleration.**<br>
*T Choudhary, V Mishra, A Goswami, et al.*<br>
Artificial Intelligence Review, 2020.

**Model Compression and Hardware Acceleration for Neural Networks A Comprehensive Survey.**<br>
*L Deng, G Li, S Han, L Shi, Y Xie.*<br> 
Proceedings of the IEEE, 2020.

**Tencent/PocketFlow.**<br>
*An Automatic Model Compression (AutoMC) framework for developing smaller and faster AI applications.*<br>
[[Github](https://github.com/Tencent/PocketFlow)]

**PaddlePaddle/PaddleSlim.**<br>
*PaddleSlim is an open-source library for deep model compression and architecture search.*<br>
[[Github](https://github.com/PaddlePaddle/PaddleSlim)]

**alibaba/TinyNeuralNetwork.**<br>
*TinyNeuralNetwork is an efficient and easy-to-use deep learning model compression framework.*<br>
[[Github](https://github.com/alibaba/TinyNeuralNetwork)]

#### Embedded Machine Learning

**Tinyml-enabled frugal smart objects: Challenges and opportunities.**<br>
*R Sanchez-Iborra, AF Skarmeta.*<br>
IEEE Circuits and Systems Magazine, 2020.

**Mcunet: Tiny deep learning on iot devices.**<br>
*J Lin, WM Chen, Y Lin, C Gan, et al.*<br>
NeurIPS, 2020.

**EtinyNet: Extremely Tiny Network for TinyML.**<br>
*K Xu, Y Li, H Zhang, R Lai, L Gu.*<br>
AAAI, 2021.

**Mcunetv2: Memory-efficient patch-based inference for tiny deep learning.**<br>
*J Lin, WM Chen, H Cai, C Gan, S Han.*<br>
ArXiv, 2021.

**Mlperf tiny benchmark.**<br>
*C Banbury, VJ Reddi, P Torelli, J Holleman, et al.*<br>
ArXiv, 2021.

**Micronets: Neural network architectures for deploying tinyml applications on commodity microcontrollers.**<br>
*C Banbury, C Zhou, I Fedorov, et al.*<br>
MLSys, 2021.

**TensorFlow lite micro: Embedded machine learning for tinyml systems.**<br>
*R David, J Duke, A Jain, et al.*<br>
MLSys, 2021.

**Network Augmentation for Tiny Deep Learning.**<br>
*C Banbury, VJ Reddi, P Torelli, J Holleman, et al.*<br>
ICLR, 2022.

**Enable deep learning on mobile devices: Methods, systems, and applications.**<br>
*H Cai, J Lin, Y Lin, Z Liu, H Tang, H Wang, et al.*<br>
TODAES, 2022.

**awesome-emdl**<br> 
*Embedded and mobile deep learning research resources.*<br>
[[Github](https://github.com/csarron/awesome-emdl)]

**gigwegbe/tinyml-papers-and-projects**<br> 
*This is a list of interesting papers and projects about TinyML.*<br>
[[Github](https://github.com/gigwegbe/tinyml-papers-and-projects)]

### Training Accelerations

#### Few Shot Learning

**Few-shot Object Detection via Feature Reweighting.**<br>
*B Kang, Z Liu, X Wang, F Yu, et al.*<br>
ICCV, 2019.
[[Github](https://github.com/bingykang/Fewshot_Detection)]

**Generalizing from a few examples A survey on few-shot learning.**<br>
*Y Wang, Q Yao, JT Kwok, LM Ni.*<br>
CSUR, 2020.

**Meta-learning of neural architectures for few-shot learning.**<br>
*T Elsken, B Staffler, JH Metzen, et al.*<br>
CVPR, 2020.

**Meta-Learning-Papers**<br>
*Meta Learning / Learning to Learn / One Shot Learning / Few Shot Learning.*<br>
[[Github](https://github.com/floodsung/Meta-Learning-Papers)]

#### Data Annotation/Labeling

**Semi-Automated Data Labeling.**<br>
*M Desmond, E DuesterwaldÔºået al.*<br>
NeurIPS, 2020.

**Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface.**<br>
*M Desmond, M Muller, Z Ashktorab, C Dugan, et al.*<br>
ICIUR, 2021.

**Semi-automatic data annotation guided by feature space projection.**<br>
*BC Benato, JF Gomes, AC Telea, AX Falc√£o.*<br>
Pattern Recognition, 2021.

**LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision.**<br>
*Z Liu, S Stent, J Li, J Gideon, et al.*<br>
ICCV, 2021.

**heartexlabs/label-studio**<br>
*Label Studio is a multi-type data labeling and annotation tool with standardized output format.*<br>
[[Github](https://github.com/heartexlabs/label-studio)]

#### Efficient Training

**Differentiable augmentation for data-efficient gan training.**<br>
*S Zhao, Z Liu, J Lin, JY Zhu, et al.*<br>
NeurlPS, 2020.

**TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning.**<br>
*H Cai, C Gan, L Zhu, S Han.*<br>
NeurIPS, 2020.

**Bag of Tricks for Adversarial Training.**<br>
*T Pang, X Yang, Y Dong, H Su, J Zhu.*<br>
ICLR, 2021.
[[Github](https://github.com/hacktfj/Bag-of-Tricks-for-AT)]

## Efficient Deep Learning Applications (EDLA)

### LiDAR

**Searching efficient 3d architectures with sparse point-voxel convolution.**<br>
*H Tang, Z Liu, S Zhao, Y Lin, J Lin, H Wang, et al.*<br>
ECCV, 2020.

**Mesorasi: Architecture support for point cloud analytics via delayed-aggregation.**<br>
*Y Feng, B Tian, T Xu, P Whatmough, et al.*<br>
MICRO, 2020.

**Efficient and robust lidar-based end-to-end navigation.**<br>
*Z Liu, A Amini, S Zhu, S Karaman et al.*<br>
ICRA, 2021.

**PointAcc: Efficient Point Cloud Accelerator.**<br>
*Y Lin, Z Zhang, H Tang, H Wang, S Han.*<br>
MICRO, 2021.

**TorchSparse: Efficient Point Cloud Inference Engine.**<br>
*H Tang, Z Liu, X Li, Y Lin, S Han.*<br>
MLSys, 2022.

### GAN

**Gan compression: Efficient architectures for interactive conditional gans.**<br>
*M Li, J Lin, Y Ding, Z Liu, JY Zhu, et al.*<br>
CVPR, 2020.

**Slimmable generative adversarial networks.**<br>
*L Hou, Z Yuan, L Huang, H Shen, X Cheng, et al.*<br>
AAAI, 2021.

**Efficient and robust lidar-based end-to-end navigation.**<br>
*Z Liu, A Amini, S Zhu, S Karaman et al.*<br>
ICRA, 2021.

### NLP

**Hat: Hardware-aware transformers for efficient natural language processing.**<br>
*H Wang, Z Wu, Z Liu, H Cai, L Zhu, C Gan, et al.*<br>
ACL, 2020.

**Efficient transformers: A survey.**<br>
*Y Tay, M Dehghani, D Bahri, D Metzler.*<br>
ArXiv, 2020.

**Lite transformer with long-short range attention.**<br>
*Z Wu, Z Liu, J Lin, Y Lin, S Han.*<br>
ICLR, 2020.

**Spatten: Efficient sparse attention architecture with cascade token and head pruning.**<br>
*H Wang, Z Zhang, S Han.*<br>
HPCA, 2021.